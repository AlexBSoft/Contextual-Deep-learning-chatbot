{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MyOwnChatbot.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8LJFlxCf4EW",
        "colab_type": "text"
      },
      "source": [
        "# Contextual Deep-learning chatbot\n",
        "\n",
        "Based on bot from [here](https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077)\n",
        "\n",
        "## How to run\n",
        "\n",
        "Writen specially for google colab.\n",
        "\n",
        "To train bot you need to upload intents.json file into /content directory. \n",
        "Sample of this file can be found at [github](https://github.com/AlexBSoft/Contextual-Deep-learning-chatbot)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCHx8ecTFitf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# things we need for NLP\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "# things we need for Tensorflow\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import tensorflow as tf\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoldjX0XFpr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import our chat-bot intents file\n",
        "import json\n",
        "with open('/content/intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0g9aFS_dFqNm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "29be273a-bdaf-4838-892f-60b08affdbb2"
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_words = ['?']\n",
        "try:\n",
        "  # loop through each sentence in our intents patterns\n",
        "  for intent in intents['intents']:\n",
        "      for pattern in intent['patterns']:\n",
        "          # tokenize each word in the sentence\n",
        "          w = nltk.word_tokenize(pattern)\n",
        "          # add to our words list\n",
        "          words.extend(w)\n",
        "          # add to documents in our corpus\n",
        "          documents.append((w, intent['tag']))\n",
        "          # add to our classes list\n",
        "          if intent['tag'] not in classes:\n",
        "              classes.append(intent['tag'])\n",
        "\n",
        "  # stem and lower each word and remove duplicates\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
        "  words = sorted(list(set(words)))\n",
        "\n",
        "  # remove duplicates\n",
        "  classes = sorted(list(set(classes)))\n",
        "\n",
        "  print (len(documents), \"documents\")\n",
        "  print (len(classes), \"classes\", classes)\n",
        "  print (len(words), \"unique stemmed words\", words)\n",
        "  \n",
        "except LookupError:\n",
        "  print ('\\033[1m',\"You need to manually download punkt using NLTK Downloader\",'\\033[0m')\n",
        "  print ('\\033[1m',\"Type 'd' and write 'punkt'\",'\\033[0m')\n",
        "  \n",
        "  nltk.download()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "45 documents\n",
            "10 classes ['opentoday', 'rental', 'today', 'дела', 'имя', 'маты', 'обо', 'пока', 'привет', 'спасибо']\n",
            "73 unique stemmed words [\"'d\", \"'s\", 'a', 'ar', 'can', 'do', 'doe', 'goodby', 'hello', 'help', 'hi', 'hour', 'how', 'i', 'lik', 'mop', 'op', 'rent', 'thank', 'that', 'thi', 'to', 'today', 'we', 'what', 'when', 'work', 'yo', 'you', 'благодарю', 'блять', 'бот', 'вечер', 'дела', 'день', 'до', 'доброе', 'добрый', 'досвиданья', 'еблан', 'желаю', 'здравия', 'здравствуйте', 'здрасьте', 'зовут', 'имя', 'как', 'какого', 'какое', 'кто', 'нахуй', 'нового', 'отец', 'пиздец', 'пока', 'привет', 'приветствую', 'свидание', 'свидания', 'свиданья', 'создал', 'создатель', 'спасибо', 'твое', 'твой', 'тебя', 'тупой', 'ты', 'утро', 'хай', 'хуила', 'хуй', 'что']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RecC7W87FtyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create our training data\n",
        "training = []\n",
        "output = []\n",
        "# create an empty array for our output\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "# training set, bag of words for each sentence\n",
        "for doc in documents:\n",
        "    # initialize our bag of words\n",
        "    bag = []\n",
        "    # list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # stem each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    # create our bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag, output_row])\n",
        "\n",
        "# shuffle our features and turn into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# create train and test lists\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "De7gT1dgFwFx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reset underlying graph data\n",
        "tf.reset_default_graph()\n",
        "# Build neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "tf.train.get_checkpoint_state('checkpoints')\n",
        "\n",
        "# Define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
        "# Start training (apply gradient descent algorithm)\n",
        "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
        "model.save('/content/model.tflearn')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_klMxDFzSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gq8L6PxYF0js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = bow(\"is your shop open today?\", words)\n",
        "print (p)\n",
        "print (classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiVlr1AoF2bD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.predict([p]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CofRy0aEF33P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save all of our data structures\n",
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRrgbnIJIRYf",
        "colab_type": "text"
      },
      "source": [
        "#Here it is our chatbot\n",
        "\n",
        "For running in production you may separate first part and this part.\n",
        "\n",
        "But don't forget to import all libraries (see the first code block)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk966jZtIYJg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# restore all of our data structures\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "import pickle\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']\n",
        "\n",
        "# import our chat-bot intents file\n",
        "import json\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQBD3PY5IdHG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build neural network\n",
        "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, 8)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
        "net = tflearn.regression(net)\n",
        "\n",
        "# Define model and setup tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYwArAMHIdpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stem each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words, show_details=False):\n",
        "    # tokenize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print (\"found in bag: %s\" % w)\n",
        "\n",
        "    return(np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuhTnTXVIf-X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = bow(\"is your shop open today?\", words)\n",
        "print (p)\n",
        "print (classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkoH4YqiIjKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load our saved model\n",
        "\n",
        "model.load('/content/model.tflearn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygPb1omZIkk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a data structure to hold user context\n",
        "context = {}\n",
        "\n",
        "ERROR_THRESHOLD = 0.25\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID='123', show_details=False):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # set context for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print ('context:', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "                    # check if this intent is contextual and applies to this user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details: print ('tag:', i['tag'])\n",
        "                        # a random response from the intent\n",
        "                        #return print(random.choice(i['responses']))\n",
        "                        return random.choice(i['responses'])\n",
        "\n",
        "            results.pop(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbZX0rkVInZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now some tests\n",
        "\n",
        "# Simple chat\n",
        "while 1:\n",
        "  print(\"> \", response(input()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9CKMuJuiThq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier\n",
        "\n",
        "while 1:\n",
        "  classify(input())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}